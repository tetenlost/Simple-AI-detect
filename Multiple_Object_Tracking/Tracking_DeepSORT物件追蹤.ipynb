{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb92cn-TBKjV"
      },
      "source": [
        "# DEEP SORT 物件追蹤\n",
        "\n",
        "  2024/07/03  \n",
        " * numpy 1.24後移除 np.float 我真的是會謝\n",
        " * 我懶得加軌跡了這程式太舊了一堆東西要改  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKuIOtnwBl5G"
      },
      "source": [
        "## 下載套件\n",
        "\n",
        "這邊將numpy版本下調至1.23安裝完成後系統會跳出視窗詢問是否重啟，請選擇重啟  \n",
        "安裝資訊中會有顯示有套件不支援,不用理它，我們這次用不到"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oaN_Hmxzhcc"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/nwojke/deep_sort.git\n",
        "!wget https://motchallenge.net/sequenceVideos/MOT16-01-raw.mp4\n",
        "!gdown --id 1bB66hP9voDXuoBoaCcKYY7a8IYzMMs4P --output deep.pb\n",
        "%pip install ultralytics\n",
        "%pip install numpy==1.23.5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 程式有以下兩處要改:\n",
        "1. /content/deep_sort/deep_sort/linear_assignment.py  \n",
        "    第4行改為\n",
        "    ```python=4\n",
        "    from scipy.optimize import linear_sum_assignment as linear_assignment\n",
        "    ```\n",
        "    58行刪除，並貼上以下程式\n",
        "    ```\n",
        "    x,y = linear_assignment(cost_matrix)\n",
        "    indices = np.array(list(zip(x, y)))\n",
        "    ```\n",
        "\n",
        "2. /content/deep_sort/deep_sort/track.py  \n",
        "    第81行，添加\n",
        "    ```\n",
        "    import random\n",
        "    self.color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
        "    ```\n",
        "\n",
        "改完記得使用ctrl + s 進行存檔"
      ],
      "metadata": {
        "id": "T9-NKhUCRssu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 宣告deep sort 特徵提取函式\n",
        "\n",
        "原生deep sort 是先將特徵提取結果存在npy檔內，再由物件追蹤算法進行讀取並追蹤\n",
        "因此必須要稍微改造一下"
      ],
      "metadata": {
        "id": "6mi4DHn7PASP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vim: expandtab:ts=4:sw=4\n",
        "import os\n",
        "import errno\n",
        "import argparse\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def _run_in_batches(f, data_dict, out, batch_size):\n",
        "    data_len = len(out)\n",
        "    num_batches = int(data_len / batch_size)\n",
        "\n",
        "    s, e = 0, 0\n",
        "    for i in range(num_batches):\n",
        "        s, e = i * batch_size, (i + 1) * batch_size\n",
        "        batch_data_dict = {k: v[s:e] for k, v in data_dict.items()}\n",
        "        out[s:e] = f(batch_data_dict)\n",
        "    if e < len(out):\n",
        "        batch_data_dict = {k: v[e:] for k, v in data_dict.items()}\n",
        "        out[e:] = f(batch_data_dict)\n",
        "\n",
        "\n",
        "def extract_image_patch(image, bbox, patch_shape):\n",
        "    \"\"\"Extract image patch from bounding box.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    image : ndarray\n",
        "        The full image.\n",
        "    bbox : array_like\n",
        "        The bounding box in format (x, y, width, height).\n",
        "    patch_shape : Optional[array_like]\n",
        "        This parameter can be used to enforce a desired patch shape\n",
        "        (height, width). First, the `bbox` is adapted to the aspect ratio\n",
        "        of the patch shape, then it is clipped at the image boundaries.\n",
        "        If None, the shape is computed from :arg:`bbox`.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray | NoneType\n",
        "        An image patch showing the :arg:`bbox`, optionally reshaped to\n",
        "        :arg:`patch_shape`.\n",
        "        Returns None if the bounding box is empty or fully outside of the image\n",
        "        boundaries.\n",
        "\n",
        "    \"\"\"\n",
        "    bbox = np.array(bbox)\n",
        "    if patch_shape is not None:\n",
        "        # correct aspect ratio to patch shape\n",
        "        target_aspect = float(patch_shape[1]) / patch_shape[0]\n",
        "        new_width = target_aspect * bbox[3]\n",
        "        bbox[0] -= (new_width - bbox[2]) / 2\n",
        "        bbox[2] = new_width\n",
        "\n",
        "    # convert to top left, bottom right\n",
        "    bbox[2:] += bbox[:2]\n",
        "    bbox = bbox.astype(np.int32)\n",
        "\n",
        "    # clip at image boundaries\n",
        "    bbox[:2] = np.maximum(0, bbox[:2])\n",
        "    bbox[2:] = np.minimum(np.asarray(image.shape[:2][::-1]) - 1, bbox[2:])\n",
        "    if np.any(bbox[:2] >= bbox[2:]):\n",
        "        return None\n",
        "    sx, sy, ex, ey = bbox\n",
        "    image = image[sy:ey, sx:ex]\n",
        "    image = cv2.resize(image, tuple(patch_shape[::-1]))\n",
        "    return image\n",
        "\n",
        "\n",
        "class ImageEncoder(object):\n",
        "\n",
        "    def __init__(self, checkpoint_filename, input_name=\"images\",\n",
        "                 output_name=\"features\"):\n",
        "        self.session = tf.compat.v1.Session()\n",
        "        with tf.io.gfile.GFile(checkpoint_filename, \"rb\") as file_handle:\n",
        "            graph_def = tf.compat.v1.GraphDef()\n",
        "            graph_def.ParseFromString(file_handle.read())\n",
        "        tf.import_graph_def(graph_def, name=\"net\")\n",
        "        self.input_var = tf.compat.v1.get_default_graph().get_tensor_by_name(\n",
        "            \"%s:0\" % input_name)\n",
        "        self.output_var = tf.compat.v1.get_default_graph().get_tensor_by_name(\n",
        "            \"%s:0\" % output_name)\n",
        "\n",
        "        assert len(self.output_var.get_shape()) == 2\n",
        "        assert len(self.input_var.get_shape()) == 4\n",
        "        self.feature_dim = self.output_var.get_shape().as_list()[-1]\n",
        "        self.image_shape = self.input_var.get_shape().as_list()[1:]\n",
        "\n",
        "    def __call__(self, data_x, batch_size=32):\n",
        "        out = np.zeros((len(data_x), self.feature_dim), np.float64)\n",
        "        _run_in_batches(\n",
        "            lambda x: self.session.run(self.output_var, feed_dict=x),\n",
        "            {self.input_var: data_x}, out, batch_size)\n",
        "        return out\n",
        "\n",
        "\n",
        "def create_box_encoder(model_filename, input_name=\"images\",\n",
        "                       output_name=\"features\", batch_size=32):\n",
        "    image_encoder = ImageEncoder(model_filename, input_name, output_name)\n",
        "    image_shape = image_encoder.image_shape\n",
        "\n",
        "    def encoder(image, boxes):\n",
        "        image_patches = []\n",
        "        for box in boxes:\n",
        "            patch = extract_image_patch(image, box, image_shape[:2])\n",
        "            if patch is None:\n",
        "                print(\"WARNING: Failed to extract image patch: %s.\" % str(box))\n",
        "                patch = np.random.uniform(\n",
        "                    0., 255., image_shape).astype(np.uint8)\n",
        "            image_patches.append(patch)\n",
        "        image_patches = np.asarray(image_patches)\n",
        "        return image_encoder(image_patches, batch_size)\n",
        "\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def generate_detections(encoder,image,rows):\n",
        "    \"\"\"Generate detections with features.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    encoder : Callable[image, ndarray] -> ndarray\n",
        "        The encoder function takes as input a BGR color image and a matrix of\n",
        "        bounding boxes in format `(x, y, w, h)` and returns a matrix of\n",
        "        corresponding feature vectors.\n",
        "    mot_dir : str\n",
        "        Path to the MOTChallenge directory (can be either train or test).\n",
        "    output_dir\n",
        "        Path to the output directory. Will be created if it does not exist.\n",
        "    detection_dir\n",
        "        Path to custom detections. The directory structure should be the default\n",
        "        MOTChallenge structure: `[sequence]/det/det.txt`. If None, uses the\n",
        "        standard MOTChallenge detections.\n",
        "\n",
        "    \"\"\"\n",
        "    features = encoder(image, rows[:, 0:4].copy())\n",
        "    detections_out = [np.r_[(row, feature)] for row, feature in zip(rows, features)]\n",
        "    return detections_out\n"
      ],
      "metadata": {
        "id": "nfH3HlNjicIC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  宣告畫框程式"
      ],
      "metadata": {
        "id": "FTlYFPEWQr2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_track_box(track,img):\n",
        "    i = len(track.tracks)\n",
        "    for trk in reversed(track.tracks):\n",
        "        d = trk.to_tlbr()\n",
        "        if (trk.time_since_update < 1) and (trk.hits >= track.n_init):\n",
        "            cv2.rectangle(img, (int(d[0]), int(d[1])), (int(d[2]),int(d[3])), trk.color, 2)\n",
        "            cv2.putText(img, str(trk.track_id), (int(d[0])-20, int(d[1])-20), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "              1, (trk.color), 1, cv2.LINE_AA)\n",
        "        i -= 1\n",
        "    return img\n",
        "    pass"
      ],
      "metadata": {
        "id": "7DxH-zcXNPDx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gc8Bc8jLvLb"
      },
      "source": [
        "## 使用YOLOV8進行偵測，並使用偵測結果進行追蹤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci-edRiSMbcM"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "from deep_sort.deep_sort import nn_matching\n",
        "from deep_sort.deep_sort.detection import Detection\n",
        "from deep_sort.deep_sort.tracker import Tracker\n",
        "\n",
        "# Load a model\n",
        "model = YOLO(\"yolov8n.pt\")  # pretrained YOLOv8n model\n",
        "cap = cv2.VideoCapture('./MOT16-01-raw.mp4')\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter('output.avi', fourcc, 20.0, (640,480))\n",
        "encoder = create_box_encoder(\"deep.pb\")\n",
        "metric = nn_matching.NearestNeighborDistanceMetric(\n",
        "    \"cosine\", 0.2, None)\n",
        "tracker = Tracker(metric)\n",
        "# Run batched inference on a list of images\n",
        "while True:\n",
        "    ret,img = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    result = model([img],conf=0.3)[0]\n",
        "# Process results list\n",
        "    datas = np.asarray(result.boxes.data.cpu())\n",
        "    datas[:,2:4]=np.asarray(result.boxes.xywh.cpu())[:,2:4]\n",
        "    datas = datas[np.where(datas[:,-1]==0)]\n",
        "    features = generate_detections(encoder,img,datas)\n",
        "    track_data = []\n",
        "    for i,feature in enumerate(features):\n",
        "        box_xywh = feature[0:4]\n",
        "        box_conf = feature[4]\n",
        "        track_data.append(Detection(box_xywh,datas[i][4],feature[6:]))\n",
        "\n",
        "    tracker.predict()\n",
        "    tracker.update(track_data)\n",
        "    img = draw_track_box(tracker,img)\n",
        "    out.write(cv2.resize(img,(640,480)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 補充：如果改完成程式後沒有反應\n",
        "那可能是因為你已經先執行過一次了，colab 為了節省計算資源不會再載入程式因此請手動載入一次\n",
        "\n",
        "# 超級麻煩"
      ],
      "metadata": {
        "id": "tocZWlAEURq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "from deep_sort import deep_sort\n",
        "importlib.reload(deep_sort.detection)\n",
        "importlib.reload(deep_sort.iou_matching)\n",
        "importlib.reload(deep_sort.kalman_filter)\n",
        "importlib.reload(deep_sort.tracker)\n",
        "importlib.reload(deep_sort.nn_matching)\n",
        "importlib.reload(deep_sort.linear_assignment)\n",
        "importlib.reload(deep_sort.track)"
      ],
      "metadata": {
        "id": "lRgbApXATNcm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}